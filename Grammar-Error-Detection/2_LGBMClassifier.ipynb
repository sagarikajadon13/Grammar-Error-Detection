{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagarikajadon13/Grammar-Error-Detection/blob/main/2_LGBMClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UQlKUtC5Sb0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f54febdb-4f39-46ca-c2da-f1a6f71a26c2"
      },
      "id": "UQlKUtC5Sb0e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c0ea2ce",
      "metadata": {
        "id": "2c0ea2ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1687451-a145-44b7-a8c8-16371c7f8c85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import spacy\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "import lightgbm as lgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5eba5ea5",
      "metadata": {
        "id": "5eba5ea5"
      },
      "outputs": [],
      "source": [
        "train= pd.read_csv('/content/drive/MyDrive/SHL NLP task/train_processed.csv')\n",
        "val= pd.read_csv('/content/drive/MyDrive/SHL NLP task/val_data.csv')\n",
        "test= pd.read_csv('/content/drive/MyDrive/SHL NLP task/test_data.csv')\n",
        "test_inputs= test['input']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57770d18",
      "metadata": {
        "id": "57770d18"
      },
      "outputs": [],
      "source": [
        "X_train= train['input']\n",
        "y_train= train['labels']\n",
        "\n",
        "X_val= val['input']\n",
        "y_val= val['labels']\n",
        "\n",
        "test= test['input']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer= TfidfVectorizer(lowercase= False, tokenizer= sent_tokenize)\n",
        "vectorizer.fit(X_train)\n",
        "X_train= vectorizer.transform(X_train)\n",
        "X_val= vectorizer.transform(X_val)\n",
        "test= vectorizer.transform(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsW5WQAoux62",
        "outputId": "5d8255ac-607d-4d7b-a743-87a6ae7aaf85"
      },
      "id": "dsW5WQAoux62",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model= lgb.LGBMClassifier(objective= 'binary',\n",
        "                          max_depth= 3,\n",
        "                          learning_rate= 0.1,\n",
        "                          n_estimators= 1000)"
      ],
      "metadata": {
        "id": "B9fkuyTQux4V"
      },
      "id": "B9fkuyTQux4V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, y_train,\n",
        "          eval_set= [(X_val, y_val)],\n",
        "          eval_metric= 'logloss',\n",
        "          early_stopping_rounds= 100,\n",
        "          verbose= 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dIC7Ffa7ux0s",
        "outputId": "ff03c957-8999-4b85-bffe-6428db7d927e"
      },
      "id": "dIC7Ffa7ux0s",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training until validation scores don't improve for 100 rounds.\n",
            "[100]\tvalid_0's binary_logloss: 0.693566\tvalid_0's binary_logloss: 0.693566\n",
            "Early stopping, best iteration is:\n",
            "[2]\tvalid_0's binary_logloss: 0.693133\tvalid_0's binary_logloss: 0.693133\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LGBMClassifier(max_depth=3, n_estimators=1000, objective='binary')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred= model.predict(X_val)\n",
        "f1_score(y_val, y_pred)"
      ],
      "metadata": {
        "id": "C4iXoqJBfEhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4797b10-ee79-4c91-fe8d-19b645be682a"
      },
      "id": "C4iXoqJBfEhg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6641061923051007"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds_lgbm= model.predict(test)"
      ],
      "metadata": {
        "id": "Jk2FkMdajogG"
      },
      "id": "Jk2FkMdajogG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub= pd.DataFrame({'input': test_inputs,\n",
        "                   'labels': test_preds_lgbm})"
      ],
      "metadata": {
        "id": "ktEsP3hARaMV"
      },
      "id": "ktEsP3hARaMV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub.to_csv('submission_lgbm.csv', index= False)"
      ],
      "metadata": {
        "id": "RhSv7UUqaFnH"
      },
      "id": "RhSv7UUqaFnH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "colab": {
      "name": "2. LGBMClassifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
